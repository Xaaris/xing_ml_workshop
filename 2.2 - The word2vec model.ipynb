{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Word2vec is a model that maps words from a vocabulary to vectors that have typically 100 to 300 dimensions.\n",
    "\n",
    "Model training takes as input a large document collection and assigns each unique word a corresponding vector.\n",
    "\n",
    "The vectors are called **word embeddings**.\n",
    "\n",
    "Words that appear in similar contexts in the document collection have vectors that are located in close distance to each other.\n",
    "\n",
    "It can be shown that word vectors actually encode word meanings and relations between them.\n",
    "\n",
    "The interesting idea of the word2vec algorithm is that it generates the word vectors as a by-product of solving a **classification problem**.\n",
    "\n",
    "A word2vec model takes fix-sized subsequences of words from the document collection and predicts the center word. \n",
    "\n",
    "The model internally represents every word as a word vector and the training process modifies the vectors so that they encode word semantic. \n",
    "\n",
    "\n",
    "## Optimization objective\n",
    "\n",
    "We need a **vocabulary** that maps every word to a word ID $w_i$.\n",
    "\n",
    "The model is trained on a sequence of training word IDs $\\{w_1,w_2,...,w_T\\}$. This is our collection of documents.\n",
    "\n",
    "Given a **word context** $[w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k}]$ of size $2k$, the model predicts the center word $w_t$. \n",
    "\n",
    "This is a **classification problem** where the examples are the word contexts and the classes are the words in the vocabulary.\n",
    "\n",
    "More specifically, given a context, the model outputs a conditional probability distribution over all words $w_i$ in the vocabulary to be the center word $w_t$:\n",
    "\n",
    "$$P(w_i=w_t \\mid w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k})$$\n",
    "\n",
    "\n",
    "The optimization objective is to **maximize** the average probability over all examples:\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum^{T-k}_{t=k} P(w_t \\mid w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k})\n",
    "$$\n",
    "\n",
    "Note that we are only interested in the predicted probabilities of the true center word (this is a hint for cross entropy!).\n",
    "\n",
    "Maximizing a probability is the same as maximizing the log of the probability. \n",
    "\n",
    "Therefore we can change the expression to maximize the average log probability instead of the average probability:\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum^{T-k}_{t=k} log \\ P(w_t \\mid w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k})\n",
    "$$\n",
    "\n",
    "Maximizing an expression is exactly the same as minimizing the negative of the expression.\n",
    "\n",
    "So if we just put a minus sign in front of the expression and minimize it we can use the cross-entropy loss function with gradient descent to train the model.\n",
    "\n",
    "\n",
    "## How does the model work\n",
    "\n",
    "The model **input** is a vector $[w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k}]$ of size $2k$ that encodes a context.\n",
    "\n",
    "Every word from a vocabulary of size $m$ is mapped to a unique vector, represented by a column in an **embedding matrix** $W$.\n",
    "\n",
    "Lets define a function $h(w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k}; W)$ that takes a context as input and is parametrized by $W$. \n",
    "\n",
    "$h$ extracts the word vectors from $W$ and aggregates them by one of the following operations:\n",
    "\n",
    " * concatenation\n",
    " * average\n",
    " * sum\n",
    "\n",
    "<img src=\"images/word2vec.png\" height=\"250\" width=\"400\"/> \n",
    "\n",
    "Than we do the following calculation:\n",
    "\n",
    "$$\n",
    "z = U \\cdot h(w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k}; W) + b \n",
    "$$\n",
    "\n",
    "$\\cdot$ is the dot product\n",
    "\n",
    "$U$ is a weight matrix and $b$ is a bias vector.\n",
    "\n",
    "$ z = [z_1,...,z_m]$ is a vector that has as many dimensions are we have words in the vocabulary. \n",
    "\n",
    "Each $z_i$ is the unnormalized probability that $w_i$ is the center word given the context.\n",
    "\n",
    "We use the softmax function to normalize the probabilities:\n",
    "\n",
    "$$\n",
    "P(w_i \\mid w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k}) = \\frac{e^{z_i}}{\\displaystyle\\sum_{j=1}^T {e^{z_j}}}\n",
    "$$\n",
    "\n",
    "Finally we use the cross-entropy loss as optimization objective:\n",
    "\n",
    "$$\n",
    " L = - \\frac{1}{T} \\sum^{T-k}_{t=k} log \\ P(w_t \\mid w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k})\n",
    "$$\n",
    "\n",
    "$W$, $U$ and $b$ are the trainable weights of the model. \n",
    "\n",
    "Note that $W$ is a trainable parameter as well, its values are modified by the optimizer.\n",
    "\n",
    "After training is done the columns in $W$ are the vector representations of the words in the vocabulary.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "The implementation of the model is quite simple.\n",
    "\n",
    "The function $h$ will be implemented by two components:\n",
    "\n",
    " 1. An [Embedding layer](https://keras.io/layers/embeddings/) performs the lookup of the word vectors in $W$\n",
    " 2. The aggregation of the word vectors is performed by a [Concatenate](https://keras.io/layers/merge/#concatenate), [Average](https://keras.io/layers/merge/#average) or [Add](https://keras.io/layers/merge/#add) layer.\n",
    " \n",
    "The calculation $z = U \\cdot h + b$ is performed by a [Dense](https://keras.io/layers/core/#dense) layer. \n",
    "\n",
    "The Dense layer will be configured with a softmax activation.\n",
    "\n",
    "The loss will be calculated by a cross-entropy loss function.\n",
    "\n",
    "The model will be optimized by Stochastic Gradient Descent. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
