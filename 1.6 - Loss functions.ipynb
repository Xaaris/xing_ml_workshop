{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "The **loss function** measures a models prediction error. \n",
    "\n",
    "More specifically it calculates the difference between the models prediction and the true label (classification problem) or true value (regression problem). \n",
    "\n",
    "The loss function is also called **error function** or **cost function**. \n",
    "\n",
    "When we talk about \"training the model\" or \"optimizing the model\" we actually mean minimizing the loss function.\n",
    "\n",
    "The loss is always positive, and should be zero only for cases where the networkâ€™s output is correct.\n",
    "\n",
    "Note that the loss does not have a unit, it is just a number.\n",
    "\n",
    "The loss functions is useful because:\n",
    " * It provides a measure for model performance\n",
    " * It helps to monitor the training progress\n",
    "   * if the loss does not change any more the model has stopped learning\n",
    "   * if the loss is very volatile the learning process is unstable\n",
    " * It allows to select the best model from a set of model snapshots\n",
    " * The gradient of the loss function informs the optimizer how to update model parameters to reduce the prediction error.\n",
    " \n",
    "There are many different loss functions available, the choice depends on the type of prediction problem. \n",
    "\n",
    "We will look at the following loss functions:\n",
    "\n",
    " * Cross-entropy loss\n",
    " * Mean squared error (MSE)\n",
    " * Mean average error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy loss\n",
    "\n",
    "The **cross-entropy loss** is used when a model outputs a binominal or multinomial probability distribution. \n",
    "\n",
    "This loss function is usually used in **classification problems**.\n",
    "\n",
    "In general the cross entropy loss measures the **dissimilarity** between the distributions $y$ and $\\hat y$, where $y$ is the ground truth and $\\hat y$ the networks output that has been transformed by the *softmax* function. \n",
    "\n",
    "The cross entropy loss depends only on the probability of the correct class. \n",
    "\n",
    "The loss is independent of how the remaining probability is split between incorrect classes.\n",
    "\n",
    "Lets look at a Python example how cross-entropy loss is calculated:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(y, y_hat):\n",
    "    return -np.dot(y, np.log(y_hat))\n",
    "\n",
    "def one_hot(label, nb_classes):\n",
    "    y = np.zeros(nb_classes)\n",
    "    y[label] = 1\n",
    "    return y\n",
    "\n",
    "# lets y_hat be a model prediction for some input x\n",
    "y_hat = np.array([0.7, 0.2, 0.1])\n",
    "nb_classes = y_hat.shape[0]\n",
    "\n",
    "# calculate the loss for different true labels\n",
    "print('y_hat = %s' % y_hat)\n",
    "print()\n",
    "for i in range(nb_classes):\n",
    "    y = one_hot(i, nb_classes)\n",
    "    print('true_label = %i' % i)\n",
    "    print('P(y=%d|x) = %.2f' % (i, y_hat[i]))\n",
    "    print('cross entropy loss = %.4f' % cross_entropy(y, y_hat))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the scores $\\hat y$ have been transformed using the *softmax* function and represent a conditional probability distribution, increasing the probability of the correct class means decreasing the mass assigned to all the other classes.\n",
    "\n",
    "**Note:** The input to a cross-entropy loss must be a probability distribution. It can not be used with unnormalized scores: \n",
    " * the log function is not defined for negative numbers\n",
    " * for scores > 1 the loss becomes negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at a plot of the cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "y_hat_t = np.arange(0.001, 1.0, 0.005)\n",
    "loss = -np.log(y_hat_t)\n",
    "plt.plot(y_hat_t, loss)\n",
    "plt.title('Cross entropy loss')\n",
    "plt.ylabel('$L(y,\\hat y)$')\n",
    "plt.xlabel('predicted probability of the true class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the plot that the cross-entropy loss behaves as expected. When the predicted probability of the true class goes to 1 the loss goes to 0. When the predicted probability of the true class goes to 0, the loss goes to infinity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Square Error (MSE)\n",
    "\n",
    "Mean Square Error (MSE) is commonly used as loss function for regression problems. \n",
    "\n",
    "Keep in mind that for regression problems the model output is simply a scalar and not a probability.\n",
    "\n",
    "MSE is the sum of squared distances between the target values and predicted values.\n",
    "\n",
    "$$\n",
    "MSE(y, \\hat y) = \\frac{1}{N} \\sum_i^N(y_i - \\hat y_i)^2\n",
    "$$\n",
    "\n",
    "MSE is also called L2 loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.arange(0.0, 2.0, 0.005)\n",
    "y = x**2\n",
    "plt.plot(x,y)\n",
    "plt.title('Mean square error (y=0.0)')\n",
    "plt.ylabel('$L(y,\\hat y)$')\n",
    "plt.xlabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing about MSE is that during optimization the parameter updates are getting smaller as we are getting nearer to the minimum. \n",
    "\n",
    "The problem is that the loss function is sensitive to outlier examples because of the squared term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean absolute error (MAE)\n",
    "\n",
    "Mean Absolute Error (MAE) is another loss function used for regression problems. \n",
    "\n",
    "MAE is the sum of absolute differences between the target values and predicted values.\n",
    "\n",
    "$$\n",
    "MAE(y, \\hat y) = \\frac{1}{N} \\sum_i^N \\mid y_i - \\hat y_i \\mid\n",
    "$$\n",
    "\n",
    "MAE is also called L1 loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.arange(0.0, 2.0, 0.005)\n",
    "y = abs(x)\n",
    "plt.plot(x,y)\n",
    "plt.title('Mean absolute error')\n",
    "plt.ylabel('$L(y,\\hat y)$')\n",
    "plt.xlabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to MSE, MAE is not sensitive to outliers in the train data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
