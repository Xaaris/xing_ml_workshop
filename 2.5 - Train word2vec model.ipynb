{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains how to prepare the input data and train the word2vec model.\n",
    "\n",
    "## Download Wikipedia article\n",
    "\n",
    "Download english Wikipedia articles:\n",
    "\n",
    "    aws s3 cp s3://asprenger/datasets/wikipedia/enwiki.txt .\n",
    "\n",
    "\n",
    "## Preprocess Wikipedia articles\n",
    "\n",
    "The `preprocess.py` script creates the dataset for training the model.\n",
    "\n",
    "It runs the following steps:\n",
    "\n",
    " * Remove HTML, links, numbers, special characters and punctuations\n",
    " * Tokenize the text\n",
    " * Remove stopwords and short tokens\n",
    " * Create vocabulary from the top `vocab-size` words\n",
    " * Save vocabulary to `vocab.pkl`\n",
    " * Map tokens to token IDs\n",
    " * Create examples `x` and labels `y` by sliding a window of size `win-size` over each article\n",
    " * Save examples and labels to `dataset.hdf5`\n",
    "\n",
    "Preprocess articles:\n",
    "\n",
    "    python preprocess.py --input-path enwiki.txt \\\n",
    "        --stopword-path stopwords_english.txt \\\n",
    "        --output-path . \\\n",
    "        --win-size 11 \\\n",
    "        --vocab-size 10000\n",
    "\n",
    "    Loading stopwords: stopwords_english.txt\n",
    "    Build vocabulary\n",
    "    1000 articles added to dictionary\n",
    "    2000 articles added to dictionary\n",
    "    3000 articles added to dictionary\n",
    "    4000 articles added to dictionary\n",
    "    ...\n",
    "    861000 articles added to dictionary\n",
    "    862000 articles added to dictionary\n",
    "    863000 articles added to dictionary\n",
    "    864000 articles added to dictionary\n",
    "    num words: 1687212\n",
    "    num_documents: 864785\n",
    "    num words: 10000\n",
    "    num_documents: 864785\n",
    "    1000 articles tokenized\n",
    "    2000 articles tokenized\n",
    "    3000 articles tokenized\n",
    "    4000 articles tokenized\n",
    "    ...\n",
    "\n",
    "Preprocessing creates the following files:\n",
    "\n",
    " * vocab.txt - A Gensim dictionary in text format\n",
    " * vocab.pkl - A Gensim dictionary in binary format\n",
    " * dataset.hdf5 - A hdf5 file with keys `x_train` and `y_train`\n",
    "\n",
    "The `vocab.txt` is not required for model training but very helpful for manual inspection of the vocabulary.\n",
    "\n",
    "The first line contains the total number of documents. The rest of the file contains tokenID, token and the number of documents the token appears in.\n",
    "\n",
    "    > head vocab.txt\n",
    "    864785\n",
    "    1179    a&m 1287\n",
    "    9157    aa  1397\n",
    "    7649    aaron   2430\n",
    "    8832    ab  1250\n",
    "    6890    abandon 1700\n",
    "    2549    abandoned   8447\n",
    "    8322    abbey   2954\n",
    "    8814    abbot   1104\n",
    "    2321    abbreviated 2529\n",
    "\n",
    "The dataset `dataset.hdf5` is generated by a token window $\\{w_1,...,w_{k-1},w_k,w_{k+1},...,w_{2k+1}\\}$ that slides over the text. \n",
    "\n",
    "Each window position generates an example $x=\\{w_1,...,w_{k-1},w_{k+1},...,w_{2k+1}\\}$ and $y=w_k$.\n",
    "\n",
    "**Note:** Because of the sliding window, consecutive examples in the dataset are correlated. It is important to shuffle the dataset before feeding it to the optimizer. \n",
    "\n",
    "### Notes about preprocessing\n",
    "\n",
    "The preparation of the train data has usually a significant impact on the training process, in terms of efficency and model performance.\n",
    "\n",
    "Here are some improvements for this example:\n",
    "\n",
    " * Smarter removal of stopwords, non-words, special characters etc.\n",
    " * Treat numbers, dates, years etc as special tokens\n",
    " * Use a larger vocabulary. Currently only the top 10K words are used out of a total of 1.6M words\n",
    " * Use a statistic parser to generate more accurate tokenization results\n",
    " * Use stemming or lemmatization to transform words to there base form\n",
    " * Entity extraction - merge tokens that form a person name, city, country etc to a single token. Example merge \"New\" and \"York\" to \"New York\"\n",
    "\n",
    "## Train model \n",
    "\n",
    "\n",
    "\n",
    "    python train.py --dataset-path dataset.hdf5 --vocab-path vocab.pkl --models-path /tmp/w2v_models\n",
    "\n",
    "    Using TensorFlow backend.\n",
    "    Load vocabulary\n",
    "    vocab_size: 10000\n",
    "    Load dataset.hdf5\n",
    "    X_train.shape: (68820132, 10)\n",
    "    y_train.shape: (68820132,)\n",
    "    Cutoff train data to 10000000 examples\n",
    "    X_train.shape: (10000000, 10)\n",
    "    y_train.shape: (10000000,)\n",
    "    Shuffle dataset\n",
    "    win_size: 10\n",
    "    epoch 0: loss=8.908036 acc=0.004845 time=758975.000000\n",
    "    Save model: /tmp/w2v_models/20181008_081335_8370637/w2v_model.h5\n",
    "    epoch 1: loss=8.561957 acc=0.012169 time=756866.000000\n",
    "    Save model: /tmp/w2v_models/20181008_082612_4173529/w2v_model.h5\n",
    "    ...\n",
    "    \n",
    "The model has been trained on 10M examples on a single NVIDIA Tesla K80 card for 100 epochs. Each epoch took about 12 minutes, total training time has been about 20 hours."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
