{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing\n",
    "\n",
    "Download english stopwords:\n",
    "\n",
    "    aws s3 cp s3://guj-mjum/datasets/wikipedia/enwiki.txt .\n",
    "\n",
    "Download english Wikipedia articles:\n",
    "\n",
    "    aws s3 cp s3://guj-mjum/datasets/word2vec/stopwords_english.txt .\n",
    "\n",
    "Preprocess articles:\n",
    "\n",
    "    python preprocess.py --input-path enwiki.txt \\\n",
    "        --stopword-path stopwords_english.txt \\\n",
    "        --output-path . \\\n",
    "        --win-size 11 \\\n",
    "        --vocab-size 10000\n",
    "\n",
    "    Loading stopwords: stopwords_english.txt\n",
    "    Build vocabulary\n",
    "    1000 articles added to dictionary\n",
    "    2000 articles added to dictionary\n",
    "    3000 articles added to dictionary\n",
    "    4000 articles added to dictionary\n",
    "    ...\n",
    "    861000 articles added to dictionary\n",
    "    862000 articles added to dictionary\n",
    "    863000 articles added to dictionary\n",
    "    864000 articles added to dictionary\n",
    "    num words: 1687212\n",
    "    num_documents: 864785\n",
    "    num words: 10000\n",
    "    num_documents: 864785\n",
    "    1000 articles tokenized\n",
    "    2000 articles tokenized\n",
    "    3000 articles tokenized\n",
    "    4000 articles tokenized\n",
    "    ...\n",
    "\n",
    "Preprocessing does the following tasks:\n",
    "\n",
    " * Remove HTML, links, numbers, special characters and punctuations\n",
    " * Tokenize the text\n",
    " * Remove stopwords and short tokens\n",
    " * Create vocabulary from the top `vocab-size` words\n",
    " * Save vocabulary to a file\n",
    " * Map tokens to token IDs\n",
    " * Create examples x and label y by sliding a window of size `win-size` over each article\n",
    " * Write examples and labels to a file\n",
    " \n",
    "Preprocessing creates the following files:\n",
    "\n",
    " * vocab.txt - A Gensim dictionary in text format\n",
    " * vocab.pkl - A Gensim dictionary in binary format\n",
    " * dataset.hdf5 - A hdf5 file with keys `x_train` and `y_train`\n",
    "\n",
    "The `vocab.txt` is not required later on and is generated for manual inspection of the vocabulary.\n",
    "\n",
    "The first line contains the total number of documents. The rest of the file contains tokenID, token and \n",
    "the number of documents the token appears in.\n",
    "\n",
    "    > head vocab.txt\n",
    "    864785\n",
    "    1179    a&m 1287\n",
    "    9157    aa  1397\n",
    "    7649    aaron   2430\n",
    "    8832    ab  1250\n",
    "    6890    abandon 1700\n",
    "    2549    abandoned   8447\n",
    "    8322    abbey   2954\n",
    "    8814    abbot   1104\n",
    "    2321    abbreviated 2529\n",
    "\n",
    "## Train model \n",
    "\n",
    "The training data is generated from token windows $\\{w_1,...,w_{k-1},w_k,w_{k+1},...,w_{2k+1}\\}$ where \n",
    "$x=\\{w_1,...,w_{k-1},w_{k+1},...,w_{2k+1}\\}$ and $y=w_k$.\n",
    "\n",
    "    python train.py --dataset-path dataset.hdf5 --vocab-path vocab.pkl --models-path /tmp/w2v_models\n",
    "\n",
    "    Using TensorFlow backend.\n",
    "    Load vocabulary\n",
    "    vocab_size: 10000\n",
    "    Load dataset.hdf5\n",
    "    X_train.shape: (68820132, 10)\n",
    "    y_train.shape: (68820132,)\n",
    "    Cutoff train data to 10000000 examples\n",
    "    X_train.shape: (10000000, 10)\n",
    "    y_train.shape: (10000000,)\n",
    "    Shuffle dataset\n",
    "    win_size: 10\n",
    "    epoch 0: loss=8.908036 acc=0.004845 time=758975.000000\n",
    "    Save model: /tmp/w2v_models/20181008_081335_8370637/w2v_model.h5\n",
    "    epoch 1: loss=8.561957 acc=0.012169 time=756866.000000\n",
    "    Save model: /tmp/w2v_models/20181008_082612_4173529/w2v_model.h5\n",
    "    ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
