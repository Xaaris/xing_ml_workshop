{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a step-by-step explanation of the word2vec model implementation and the training process.\n",
    "\n",
    "Note that this notebook is not intended to actually run the training, some of the Python cells might fail to execute.\n",
    "\n",
    "For the complete training code check out the Python script `word2vec/train.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Embedding, Lambda, Dense\n",
    "from keras.layers import Concatenate, Average, Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and shuffle training data\n",
    "\n",
    "First we load the whole training dataset into memory. \n",
    "\n",
    "To reduce the training time we will only use 10 million examples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(dataset_path, 'r')\n",
    "X_train = f['x_train'].value\n",
    "y_train = f['y_train'].value\n",
    "\n",
    "max_train_size = 10000000\n",
    "X_train = X_train[:max_train_size, :]\n",
    "y_train = y_train[:max_train_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we shuffle the dataset. \n",
    "\n",
    "This avoids the problem of feeding correlated examples into the training process and helps the optimizer to converge faster.\n",
    "\n",
    "We must make sure to shuffle examples and labels in a consistent fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_train = X_train[indices]\n",
    "y_train = y_train[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we have to define the **input shape** of the model. \n",
    "\n",
    "The input shape does not include the `batch_size` dimension.\n",
    "\n",
    "The train examples are word ID vectors $x=\\{w_1,...,w_{k-1},w_{k+1},...,w_{2k+1}\\}$ with associated target value $w_k$. \n",
    "\n",
    "$w_k$ is the word ID the model should predict given an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 10\n",
    "inputs = Input(shape=(win_size,), dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add an **Embedding layer** that maps each word ID to a word vector. \n",
    "\n",
    "The input shape is (batch_size, win_size).\n",
    "\n",
    "The output shape is (batch_size, win_size, vec_dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "vec_dims = 100\n",
    "word_vectors = Embedding(vocab_size, vec_dims)(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the Embedding layer is fed into `win_size` Lambda layers. Each Lambda layer extracts and outputs a single word vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_word_vector = [Lambda(lambda x: x[:,i,:], output_shape=(vec_dims,))(word_vectors) for i in range(win_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the word vectors are aggregated to a single vector.\n",
    "\n",
    "The **concatenation** of the word vectors preserves the word order but results in a larger model and longer training time. \n",
    "\n",
    "**Averaging** and **summation** of the word vectors destroys the word order but results in a smaller model and faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the word vectors\n",
    "aggregation_type = 'concat'\n",
    "if aggregation_type == 'concat':\n",
    "    h = Concatenate()(sliced_word_vector)\n",
    "elif aggregation_type == 'average':\n",
    "    h = Average()(sliced_word_vector)\n",
    "elif aggregation_type == 'sum':\n",
    "    h = Add()(sliced_word_vector)\n",
    "else:\n",
    "    raise ValueError('Invalid row aggregation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregated word vectors are fed into a **Dense layer** with a softmax activation function. \n",
    "\n",
    "This layer outputs the probability for each word of the vocabulary to be the center word $ P(y = w_k \\mid x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = Dense(vocab_size, activation='softmax')(h)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Model can be instantiated by specifying input and output tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the model for training it needs to be compiled.\n",
    "\n",
    "The model uses the following configuration:\n",
    "\n",
    " * Stochastic Gradient Descent optimizer\n",
    " * Cross-entropy loss function\n",
    " * Accuracy metric during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.05),\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.fit()` method trains the model for a given number of epochs. \n",
    "\n",
    "An `epoch` is one iterations over the dataset.\n",
    "\n",
    "The implementation calls `model.fit()` repeatedly in a loop. \n",
    "\n",
    "After each iteration the train progress is logged and a model snapshot is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_iterations = 100\n",
    "epochs_per_fit = 1\n",
    "batch_size = 256\n",
    "for i in range(nb_iterations):\n",
    "    h = model.fit(X_train, y_train, \n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs_per_fit)\n",
    "    \n",
    "    loss = h.history['loss'][-1]\n",
    "    acc = h.history['acc'][-1]\n",
    "    print('epoch %d: loss=%f acc=%f time=%f' % (epoch, loss, acc, mean_epoch_time))\n",
    "    \n",
    "    model.save(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes about the implementation\n",
    "\n",
    "The implementation is not very efficient because of the large number of softmax computations in the final Dense layer. This increases training time quite a lot. A more efficient implementation would use a hierarchical softmax.\n",
    "\n",
    "The training process does not evaluate the result on a test dataset. Normally this is a big mistake. In this case overfitting is not a big concern because we actually want the model to overfit. We train the model on a document corpus the size of wikipedia. This is supposed to be enough to cover the common usage of the language. There is no need for further generalization. \n",
    "\n",
    "Also we are only interested in generating 'good' word vector representations and not so much in a perfect prediction for the center words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
