{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description and demonstration of common Keras layers. For more information see [Keras layers](https://keras.io/layers/about-keras-layers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense layer\n",
    "\n",
    "A dense layer calculates $ output = activation(dot(W, input) + b)$\n",
    "\n",
    "Popular activation functions are:\n",
    " * linear\n",
    " * sigmoid\n",
    " * softmax\n",
    " * relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (?, 2)\n",
      "Output shape (?, 5)\n",
      "\n",
      "Trainable parameters:\n",
      "W.shape: (2, 5)\n",
      "b.shape: (5,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "weights = np.array([[1,2,3,4,5],[1,2,3,4,5]])\n",
    "bias = np.array([0,0,0,0,0])\n",
    "weights_and_bias = (weights, bias)\n",
    "\n",
    "inputs = Input(shape=(2,)) # each X_i has 2 dimensions\n",
    "outputs = Dense(5, activation='linear', weights=weights_and_bias)(inputs) \n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "print('Input shape', model.input.shape)\n",
    "print('Output shape', model.output.shape)\n",
    "\n",
    "model_weights = model.layers[1].get_weights()\n",
    "print('\\nTrainable parameters:')\n",
    "print(\"W.shape:\", model_weights[0].shape)\n",
    "print(\"b.shape:\", model_weights[1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Dropout layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "max_sequence = 5\n",
    "embedding_dims = 10\n",
    "\n",
    "# Size of the vocabulary. The assumption is that indexing starts with 0\n",
    "# and is consequtive.\n",
    "vocab_size = 3\n",
    "\n",
    "inputs = Input(shape=(max_sequence,), dtype='int32') # each X_i is a sequence of 'max_sequence' integers\n",
    "outputs = Embedding(vocab_size, embedding_dims, input_length=max_sequence)(inputs)\n",
    "\n",
    "model = Model([inputs], [outputs])\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "model.predict( np.array([[0,1,1,1,2]]) )\n",
    "\n",
    "# An embedding layer is like a lookup table. The values in the input \n",
    "# vector are used as indices in the internal weights matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda layer\n",
    "Simple way to add functionality to a model. Best used for stateless functions. For stateful functions it is better to implement a separate layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1.]]\n",
      "[[1. 2. 3. 4.]]\n",
      "[[[2. 2. 2. 2.]\n",
      "  [4. 4. 4. 4.]\n",
      "  [6. 6. 6. 6.]\n",
      "  [8. 8. 8. 8.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Concatenate, Lambda, Embedding, Average, Dense, Add\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "inputs = Input(shape=(4,4), dtype='float32')\n",
    "first_row = Lambda(lambda x: x[:,0,:])(inputs)\n",
    "first_column = Lambda(lambda x: x[:,:,0])(inputs)\n",
    "doubled = Lambda(lambda x: x*2.0)(inputs)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[first_row, first_column, doubled])\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "x = np.array([\n",
    "    [\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3],\n",
    "    [4,4,4,4],        \n",
    "    ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "x,y,z = model.predict(x)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate and Average layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4)\n",
      "concat_out [[1. 1. 1. 1. 2. 2. 2. 2. 3. 3. 3. 3. 4. 4. 4. 4.]]\n",
      "avg_out [[2.5 2.5 2.5 2.5]]\n",
      "sum_out [[10. 10. 10. 10.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Concatenate, Lambda, Embedding, Average, Dense, Add\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "inputs = Input(shape=(4,4), dtype='float32')\n",
    "word_vector_rows = [Lambda(lambda x: x[:,i,:])(inputs) for i in range(win_size)]\n",
    "concat_out = Concatenate()(word_vector_rows)\n",
    "avg_out = Average()(word_vector_rows)\n",
    "sum_out = Add()(word_vector_rows)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[concat_out, avg_out, sum_out])\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "x = np.array([\n",
    "    [\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3],\n",
    "    [4,4,4,4],        \n",
    "    ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "concat_out_val, avg_out_val, sum_out_val = model.predict(x)\n",
    "print('concat_out', concat_out_val)\n",
    "print('avg_out', avg_out_val)\n",
    "print('sum_out', sum_out_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax layer\n",
    "\n",
    "For a vector $x$ with elements $x_i$ the softmax function calculates:\n",
    "\n",
    "$ softmax(x_i) = \\frac{e^{x_i}}{\\sum_i {e^{x_i}}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities: [[2.6827645e-01 6.6499080e-04 7.2925097e-01 1.8076325e-03]]\n",
      "predicted class: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Concatenate, Lambda, Embedding, Average, Dense, Add, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "nb_classes = 4\n",
    "inputs = Input(shape=(nb_classes,), dtype='float32')\n",
    "softmax = Activation('softmax')(inputs)\n",
    "model = Model(inputs=inputs, outputs=softmax)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "x = np.array([[8.0, 2.0, 9.0, 3.0]])\n",
    "probs = model.predict(x)\n",
    "print('probabilities:', probs)\n",
    "print('predicted class:', np.argmax(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy loss\n",
    "\n",
    "Input to the cross entropy function **must** be a probability distribution.\n",
    "\n",
    "$ cross\\_entropy(y\\_true, y\\_pred) = -log(y\\_pred_{y\\_true}) $\n",
    "\n",
    "There are multiple implementations of cross entropy:\n",
    " * categorical vs. binary\n",
    " * sparse vs. one-hot encoded\n",
    " \n",
    "In Tensorflow you usually use optimized functions that combine softmax and cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy loss: 4.6051702\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "nb_classes = 5\n",
    "y_true = K.variable(value=np.array([1]), dtype='float32')\n",
    "y_pred = K.variable(value=np.array([[0.01, 0.01, 0.96, 0.01, 0.01]]), dtype='float32')\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "loss = K.eval(loss_fn)\n",
    "print('cross entropy loss:', loss[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
