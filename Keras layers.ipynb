{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections demonstrate the function and behavior of some Keras layers. \n",
    "\n",
    "With building a single layer model and running a forward pass (e.g. calling `predict()`) it is possible to introspect the behavior of a layer in isolation. \n",
    "\n",
    "For more information see [Keras layers](https://keras.io/layers/about-keras-layers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense layer\n",
    "\n",
    "A dense layer performs the computation `output = activation(dot(input, W) + b)`.\n",
    "\n",
    "A dense layer takes a tensor of shape (batch_size, input_size) as input and returns a tensor of shape (batch_size, output_size).\n",
    "\n",
    " * `W` is a (input_size, output_size) weight matrix \n",
    " * `b` is a output_size dim. vector\n",
    "\n",
    "Some frequently used [activation functions](https://keras.io/activations) are:\n",
    " * `linear`: identity function, e.g. no activation is applied\n",
    " * `relu`: rectified linear unit\n",
    " * `sigmoid`: Sigmoid function, used in binary classification\n",
    " * `softmax`: softmax function, used in multi-class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (?, 2)\n",
      "Output shape (?, 5)\n",
      "Output: [[ 3.  6.  9. 12. 15.]]\n",
      "Numpy result: [ 3  6  9 12 15]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "W = np.array([\n",
    "    [1,2,3,4,5],\n",
    "    [1,2,3,4,5]])\n",
    "b = np.array([0,0,0,0,0])\n",
    "weights_and_bias = (W, b)\n",
    "\n",
    "inputs = Input(shape=(2,))\n",
    "outputs = Dense(5, activation='linear', weights=weights_and_bias)(inputs) \n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "print('Input shape', model.input.shape)\n",
    "print('Output shape', model.output.shape)\n",
    "\n",
    "x = np.array([1,2])\n",
    "print('Output:', model.predict(np.expand_dims(x, 0)))\n",
    "\n",
    "np_result = np.dot(x, W) + b\n",
    "print('Numpy result:', np_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax activation\n",
    "\n",
    "For a vector $x = [x_0,...,x_n]$ the softmax function calculates:\n",
    "\n",
    "$ softmax(x_i) = \\frac{e^{x_i}}{\\sum_i {e^{x_i}}} $\n",
    "\n",
    "**Note:** In Keras any [activation function](https://keras.io/activations/) can either be used with an Activation layer, or through the activation argument in the layer constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities: [[2.6827645e-01 6.6499080e-04 7.2925097e-01 1.8076325e-03]]\n",
      "predicted class: 2\n",
      "sum of probabilities: 1.0000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "nb_classes = 4\n",
    "inputs = Input(shape=(nb_classes,), dtype='float32')\n",
    "softmax = Activation('softmax')(inputs)\n",
    "model = Model(inputs=inputs, outputs=softmax)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "activation = np.array([[8.0, 2.0, 9.0, 3.0]])\n",
    "probs = model.predict(activation)\n",
    "print('probabilities:', probs)\n",
    "print('predicted class:', np.argmax(probs))\n",
    "print('sum of probabilities:', probs.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy loss\n",
    "\n",
    "Input to the cross entropy function **must** be a probability distribution.\n",
    "\n",
    "$ cross\\_entropy(y\\_true, y\\_pred) = -log(y\\_pred_{y\\_true}) $\n",
    "\n",
    "There are multiple implementations of cross entropy:\n",
    " * categorical vs. binary\n",
    " * sparse vs. one-hot encoded\n",
    " \n",
    "In Tensorflow you usually use optimized functions that combine softmax and cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy loss: 4.6051702\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "nb_classes = 5\n",
    "y_true = K.variable(value=np.array([1]), dtype='float32')\n",
    "y_pred = K.variable(value=np.array([[0.01, 0.01, 0.96, 0.01, 0.01]]), dtype='float32')\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "loss = K.eval(loss_fn)\n",
    "print('cross entropy loss:', loss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "max_sequence = 5\n",
    "embedding_dims = 10\n",
    "\n",
    "# Size of the vocabulary. The assumption is that indexing starts with 0\n",
    "# and is consequtive.\n",
    "vocab_size = 3\n",
    "\n",
    "inputs = Input(shape=(max_sequence,), dtype='int32') # each X_i is a sequence of 'max_sequence' integers\n",
    "outputs = Embedding(vocab_size, embedding_dims, input_length=max_sequence)(inputs)\n",
    "\n",
    "model = Model([inputs], [outputs])\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "model.predict( np.array([[0,1,1,1,2]]) )\n",
    "\n",
    "# An embedding layer is like a lookup table. The values in the input \n",
    "# vector are used as indices in the internal weights matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda layer\n",
    "Simple way to add functionality to a model. Best used for stateless functions. For stateful functions it is better to implement a separate layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1.]]\n",
      "[[1. 2. 3. 4.]]\n",
      "[[[2. 2. 2. 2.]\n",
      "  [4. 4. 4. 4.]\n",
      "  [6. 6. 6. 6.]\n",
      "  [8. 8. 8. 8.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Concatenate, Lambda, Embedding, Average, Dense, Add\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "inputs = Input(shape=(4,4), dtype='float32')\n",
    "first_row = Lambda(lambda x: x[:,0,:])(inputs)\n",
    "first_column = Lambda(lambda x: x[:,:,0])(inputs)\n",
    "doubled = Lambda(lambda x: x*2.0)(inputs)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[first_row, first_column, doubled])\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "x = np.array([\n",
    "    [\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3],\n",
    "    [4,4,4,4],        \n",
    "    ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "x,y,z = model.predict(x)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge layer: concatenate, average and sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4)\n",
      "concat_out [[1. 1. 1. 1. 2. 2. 2. 2. 3. 3. 3. 3. 4. 4. 4. 4.]]\n",
      "avg_out [[2.5 2.5 2.5 2.5]]\n",
      "sum_out [[10. 10. 10. 10.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Concatenate, Lambda, Embedding, Average, Dense, Add\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "inputs = Input(shape=(4,4), dtype='float32')\n",
    "word_vector_rows = [Lambda(lambda x: x[:,i,:])(inputs) for i in range(win_size)]\n",
    "concat_out = Concatenate()(word_vector_rows)\n",
    "avg_out = Average()(word_vector_rows)\n",
    "sum_out = Add()(word_vector_rows)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[concat_out, avg_out, sum_out])\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "x = np.array([\n",
    "    [\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3],\n",
    "    [4,4,4,4],        \n",
    "    ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "concat_out_val, avg_out_val, sum_out_val = model.predict(x)\n",
    "print('concat_out', concat_out_val)\n",
    "print('avg_out', avg_out_val)\n",
    "print('sum_out', sum_out_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
