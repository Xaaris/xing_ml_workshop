{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper [Distributed Representations of Sentences and Documents](https://arxiv.org/abs/1405.4053) describes the following word2vec algorithm:\n",
    "\n",
    "In this framework, every word is mapped to a unique vector, represented by a column in a matrix $W$. The column is indexed by position of the word in the vocabulary. The concatenation or sum of the vectors is then used as features for prediction of the next word in a sentence.\n",
    "\n",
    "More formally, given a sequence of training words $w_1,w_2,...,w_T$ , the goal of the word vector model is to maximize the average log probability\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum^{T-k}_{t=k} log \\ p(w_t \\mid w_{t-k},...,w_{t+k})\n",
    "$$\n",
    "\n",
    "**Note:** The model can calculate the probability for a word to be the center word, given the k words before and after. The goal is to maximize the log probability of the correct word $w_t$. Do not be confused by the log, it makes the math simpler (especially calculating gradients) but does not change anything.\n",
    "\n",
    "\n",
    "**TODO** turn the next two formulars around: first calculate the unnormalized log probabilities and than normalize them with the softmax.\n",
    "\n",
    "\n",
    "\n",
    "The prediction task is typically done via a multiclass classifier, such as softmax. There, we have:\n",
    "\n",
    "$$\n",
    "p(w_t \\mid w_{t-k},...,w_{t+k}) = \\frac{e^{y_{w_t}}}{\\sum_i {e^{y_i}}}\n",
    "$$\n",
    "\n",
    "Each of $y_i$ is un-normalized log-probability for each output word $i$, computed as\n",
    "\n",
    "$$\n",
    "y = b + Uh(w_{t-k},...,w_{t+k}; W)\n",
    "$$\n",
    "\n",
    "where $U$, $b$ are the softmax parameters. $h$ is constructed by a concatenation or average of word vectors extracted from $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec model prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Concatenate, Lambda, Embedding, Average, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "k = 2\n",
    "vec_dims = 10\n",
    "vocab_size = 100\n",
    "row_aggregation = 'concatenate' # 'average' or 'concatenate'\n",
    "\n",
    "win_size = 2 * k\n",
    "inputs = Input(shape=(win_size,), dtype='int32') # input shape: (-1, win_size)\n",
    "word_vectors = Embedding(vocab_size, vec_dims)(inputs) # h shape: (-1, win_size, vec_dim)\n",
    "word_vector_rows = [Lambda(lambda x: x[:,i,:], output_shape=(1,vec_dims))(word_vectors) for i in range(win_size)]\n",
    "if row_aggregation == 'concatenate':\n",
    "    h = Concatenate()(word_vector_rows)\n",
    "elif row_aggregation == 'average':\n",
    "    h = Average()(word_vector_rows)\n",
    "else:\n",
    "    raise ValueError('Invalid row aggregation')\n",
    "    \n",
    "# dense_output = activation(dot(input, kernel) + bias)\n",
    "logits = Dense(vocab_size, activation='softmax')(h)\n",
    "model = Model(inputs, logits)\n",
    "\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "x = np.array([\n",
    "    [0,0,1,1],\n",
    "])\n",
    "print('x.shape:', x.shape)\n",
    "out = model.predict(x)\n",
    "print('out.shape:', out.shape)\n",
    "\n",
    "W, b = model.layers[-1].get_weights()\n",
    "print('W:', W.shape)\n",
    "print('b:', b.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
