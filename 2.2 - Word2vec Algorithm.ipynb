{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "ML algorithms require the input to be represented as a fixed-length feature vector.\n",
    "\n",
    "When it comes to texts, one of the most common fixed-length features is bag-of-words.\n",
    "\n",
    "One-hot encoding: huge vector, does not capture semantic\n",
    "\n",
    "BOW features have two major weaknesses: \n",
    " * they lose the ordering of the words\n",
    " * they also ignore semantics of the words\n",
    " \n",
    "Paragraph Vector algorithm:\n",
    " * unsupervised\n",
    " * learns from variable-length pieces of texts (sentences, paragraphs, documents etc.)\n",
    " * generates fixed-length representation\n",
    " \n",
    " \n",
    "Text classification and clustering tasks are implemented using logistic regression and k-means.\n",
    "They require fixed length input.\n",
    "Most common fixed-length vector representation for texts is the bag-of-words or bag-of-n-grams.\n",
    "\n",
    "The word order is lost, and thus different sentences can have exactly the same representation, as long as the same words are used.\n",
    "\n",
    "bag-of-n-grams considers the word order in short context but suffers from data sparsity and high dimensionality.\n",
    "\n",
    "Bag-of-words and bag- of-n-grams have very little sense about the semantics of the words or more formally the distances between the words. This means that words “powerful,” “strong” and “Paris” are equally distant despite the fact that semantically, “power- ful” should be closer to “strong” than “Paris.”\n",
    "\n",
    "While paragraph vectors are unique among paragraphs, the word vectors are shared. At prediction time, the paragraph vectors are inferred by fix- ing the word vectors and training the new paragraph vector until convergence.\n",
    "\n",
    "The outcome is that after the model is trained, the word vectors are mapped into a vector space such that semantically similar words have similar vector representa- tions (e.g., “strong” is close to “powerful”).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec algorithm\n",
    "\n",
    "We need a **vocabulary** that maps every word that appear in a document to an integer ID.\n",
    "\n",
    "Every word is mapped to a unique vector, represented by a column in a matrix $W$. The column is indexed by the word ID  from the vocabulary. \n",
    "\n",
    "The concatenation or sum of the vectors is then used as features for prediction of the next word in a sentence.\n",
    "\n",
    "<img src=\"images/word2vec.png\" height=\"250\" width=\"400\"/> \n",
    "\n",
    "**What is the goal?**\n",
    "\n",
    "The model is trained on a sequence of training words $w_1,w_2,...,w_T$. This is usually a large corpus of documents.\n",
    "\n",
    "Given a **window** $[w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k}]$, the model predicts the center word $w_t$. \n",
    "\n",
    "This is a **classification problem** where the classes are the words in the vocabulary. Each examples is a word window and the label is the center word.\n",
    "\n",
    "More specifically, given a window, the model predicts the probability for each word $w_i$ in the vocabulary to be the center word: $p(w_i \\mid w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k})$\n",
    "\n",
    "The goal of the model is to **maximize** the average probability:\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum^{T-k}_{t=k} p(w_t \\mid w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k})\n",
    "$$\n",
    "\n",
    "Instead of the average probability we maximize the average log probability:\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum^{T-k}_{t=k} log \\ p(w_t \\mid w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k})\n",
    "$$\n",
    "\n",
    "This modification does not change the optimization objective because by maximizing a log probability we also maximize the probability. With this modification we can simply use standard cross entropy as loss function and minimize the loss using a gradient descent optimizer.\n",
    "\n",
    "## How it works\n",
    "\n",
    "Every word from the vocabulary is mapped to a unique vector, represented by a column in a matrix $W$.\n",
    "\n",
    "Define a function $h(w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k}; W)$ that takes a context as input and is parametrized by $W$. \n",
    "\n",
    "$h$ extracts the word vectors from $W$ and aggregates them by one of:\n",
    " * concatenation\n",
    " * average\n",
    " * sum\n",
    "\n",
    "Than calculate:\n",
    "\n",
    "$$\n",
    "y = b + Uh(w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k}; W)\n",
    "$$\n",
    "\n",
    "$y = [y_1,...,y_m]$ is a vector that has as many dimensions are we have words in the vocabulary. Each $y_i$ is the unnormalized probability that $w_i$ is the center word given the context.\n",
    "\n",
    "$U$ is a (vocab_size, 2*k) weight matrix and $b$ is vocab_size dimensional bias vector.\n",
    "\n",
    "We use the softmax function to normalize the probabilities:\n",
    "\n",
    "$$\n",
    "p(w_i \\mid w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k}) = \\frac{e^{y_i}}{\\sum_j {e^{y_j}}}\n",
    "$$\n",
    "\n",
    "Loss function:\n",
    "\n",
    "**TODO**\n",
    "\n",
    "$$\n",
    "p(w_t \\mid w_{t-k},...,w_{t-1}, w_{t+1},...,w_{t+k}) = \\frac{e^{y_{w_t}}}{\\sum_j {e^{y_j}}}\n",
    "$$\n",
    "\n",
    "$W$, $U$ and $b$ are the trainable weights of the model. Note that $W$ is a trainable parameter as well, its values are modified by the optimizer.\n",
    "\n",
    "After training is done the columns in $W$ are the vector representations of the words in the vocabulary.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "The implementation of the model is quite simple.\n",
    "\n",
    "The function $h()$ can be implemented by two parts:\n",
    " 1. An embedding layer provides the lookup of the column vectors in $W$\n",
    " 2. Layers for concatenation, averaging or addition provide the aggregation\n",
    "\n",
    "The calculation $y = b + Uh()$ is performed by a dense layer. The dense layer will be configured with a softmax activation that normalized the probabilities.\n",
    "\n",
    "The loss will be calculated by a standard cross entropy function.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
