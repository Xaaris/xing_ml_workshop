{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive right in and train our first model!\n",
    "\n",
    "The problem we are solving here is to classify grayscale images of handwritten digits (28x28 pixels) into 10 different categories (0 through 9). \n",
    "\n",
    "We’ll use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset. \n",
    "\n",
    "It’s a set of 60,000 training images, plus 10,000 test images.\n",
    "\n",
    "Here are some example images from MNIST:\n",
    "\n",
    "<img src=\"images/mnist-1.jpg\" height=\"450\" width=\"600\"/>\n",
    "\n",
    "Note the different train and test datasets. \n",
    "\n",
    "We'll train the model on the train dataset and evaluate it on the test dataset. \n",
    "\n",
    "This separation is very important for a correct measurement of the model performance. \n",
    "\n",
    "We want to measure how well the model performs on data it has never seen before, this property is called **generalization**. \n",
    "\n",
    "At some point in the training process the model will overfit on the training data and as a result the performance on the test data will get worse.\n",
    "\n",
    "Some ML terminology:\n",
    "\n",
    " * data points are called **examples**, usually denoted as $x$ for a single example or $X$ for multiple examples\n",
    " * a category in a classification problem is called a **class**\n",
    " * the class associated with a specific sample is called a **label**, usually denoted as $y$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print('Train images:', train_images.shape)\n",
    "print('Train labels:', train_labels.shape)\n",
    "print('Test images:', test_images.shape)\n",
    "print('Test labels:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some training examples\n",
    "What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(3):\n",
    "    plt.imshow(train_images[i], cmap='gray')\n",
    "    print('Label:', train_labels[i])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_images` and `train_labels` form the training set, the data that the model will learn from. \n",
    "\n",
    "The model will then be tested on the test set, `test_images` and `test_labels`.\n",
    "\n",
    "The images are encoded as Numpy arrays, and the labels are an array of digits, ranging from 0 to 9. \n",
    "\n",
    "The images and labels have a one-to-one correspondence.\n",
    "\n",
    "The workflow will be as follows: \n",
    "\n",
    " 1. We’ll feed the neural network the training data, `train_images` and `train_labels`. \n",
    " 2. The network will then learn to associate images and labels. \n",
    " 3. We’ll ask the network to produce predictions for `test_images`, and we’ll verify whether these predictions match the labels from `test_labels`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "The core building block of neural networks is the **layer**. \n",
    "\n",
    "A layer is a function with some input and some output. \n",
    "\n",
    "In the following example, our network consists of a sequence of two **dense layers**, also sometimes called fully connected layers.\n",
    "\n",
    "A dense layer implements the following function: \n",
    "\n",
    "$$ \n",
    "output = activation(W \\cdot input + b) \n",
    "$$\n",
    "\n",
    "$input$ and $output$ are vectors.\n",
    "\n",
    "$\\cdot$ is the dot product.\n",
    "\n",
    "$W$ is a matrix and $b$ is a vector, they are called the **parameters** of the layer. \n",
    "\n",
    "When you think about a layer as a stateful function than $W$ and $b$ would be the state.\n",
    "\n",
    "The **activation function** computes the output of a layer. \n",
    "\n",
    "In most cases the activation function is a non-linear function. \n",
    "\n",
    "This model uses **Rectified Linear Unit (ReLU)** and **Softmax** activation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# define the model input\n",
    "inputs = layers.Input((28 * 28,))\n",
    "\n",
    "# define the first dense layer (hidden layer)\n",
    "x = layers.Dense(units=256, activation='relu')(inputs)\n",
    "\n",
    "# define the output layer\n",
    "output = layers.Dense(units=10, activation='softmax')(x)\n",
    "\n",
    "# define a model that holds everything together\n",
    "model = models.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer uses a softmax activation function, that returns a vector of 10 probability scores (summing to 1). \n",
    "\n",
    "Each score will be the probability that the input image belongs to one of our 10 digit classes.\n",
    "\n",
    "The actual prediction will be the class with the highest probability.\n",
    "\n",
    "In summary the model calculates the following function:\n",
    "\n",
    "$$ \n",
    "probabilities = softmax(W_2 \\cdot relu(W_1 \\cdot input + b_1) + b_2)\n",
    "$$\n",
    "\n",
    "$W_1$, $b_1$, $W_2$ and $b_2$ are the parameters of the model.\n",
    "\n",
    "Lets look at some details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some details about each layer in the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has about 200K parameters. In general this depends on the input size and the type and configuration of the individual layers. \n",
    "\n",
    "In our specific model, given that the input size and the number of classes is fixed, the number of parameters depends only on the `units` argument in the first dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Keras model has the following primary methods:\n",
    "\n",
    " * `fit()`: this trains the model by 'fitting' the model parameters to the training data\n",
    " * `evaluate()`: measure the performance of the model by calculating evaluation metrics\n",
    " * `predict()`: predict labels for a set of examples\n",
    "\n",
    "\n",
    "To be able to train the model we need three more things:\n",
    "\n",
    " * **loss function -** this measures the models prediction error, the difference between the prediction and the ground truth.\n",
    " * **optimizer -** an algorithm through which the model will update its parameters based on the training examples it sees and the loss calculated by the loss function\n",
    " * **other metrics -** in this case we only care about **accuracy**, the fraction of the images that were correctly classified\n",
    " \n",
    "The model must be compiled to be ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Before we can start the training we need to preprocess the images. \n",
    "\n",
    "The model expects each train/test example as a vector, therefore we flatten the images. \n",
    "\n",
    "We also transform the image values to the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before preprocessing:')\n",
    "print('Train images:', train_images.shape, train_images.dtype)\n",
    "print('Test images:', test_images.shape, test_images.dtype)\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "print('\\nAfter preprocessing:')\n",
    "print('Train images:', train_images.shape, train_images.dtype)\n",
    "print('Test images:', test_images.shape, test_images.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re now ready to train the network. This is done by calling the models `fit()` method, it fits the model to its training data. \n",
    "\n",
    "`fit()` takes the following arguments:\n",
    "\n",
    " * train_images - the examples to train on\n",
    " * train_labels - the labels to calculate the loss\n",
    " * epochs - number of iterations over the training data\n",
    " * batch_size - number of examples to process in one step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two quantities are displayed during training: \n",
    " * `loss`: the loss of the network over the training data\n",
    " * `acc`: the accuracy of the network over the training data\n",
    " \n",
    "Now lets check how the model performs on the test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is the percentage of correct predictions. \n",
    "\n",
    "The test accuracy might be lower than the train accuracy. \n",
    "\n",
    "This is expected and called **overfitting** - during training the model learns aspects of the examples that are specific for the training data.\n",
    "\n",
    "On a realworld problem it is usually not possible to train a model with perfect test accuracy. \n",
    "\n",
    "Having a test accuracy close to 1.0 usually means you have a bug in your preprocessing or training code and the model does not generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a confusion matrix\n",
    "The accuracy metric is the mean accuracy over all classes. \n",
    "\n",
    "The **confusion matix** is a tool to better understand the predictive performance of a model on individual classes. \n",
    "\n",
    "It breaks the evaluation result down and shows the correct and incorrect predictions for individual classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "probabilities = model.predict(test_images)\n",
    "predictions = np.argmax(probabilities, axis=1)\n",
    "confusion_matrix = pd.crosstab(test_labels, predictions, rownames=['True'], colnames=['Predicted'], margins=False)\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vertical axis represents the true labels and the horizontal axis represents the predicted labels. \n",
    "\n",
    "The values in the matrix are the counts how many times a true label has been predicted as one of the 10 classes. \n",
    "\n",
    "The main diagonal are the correct predictions, this is where a good model will have most of the counts.\n",
    "\n",
    "In the example you can see that `4` and `9` are often confused with each other. \n",
    "\n",
    "This makes sense because the two digits have a similar shape. \n",
    "\n",
    "On the other hand `0` and `1` are never confused with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(df_confusion, cmap=plt.cm.YlOrRd):\n",
    "    plt.matshow(df_confusion, cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "    plt.yticks(tick_marks, df_confusion.index)\n",
    "    plt.ylabel(df_confusion.index.name)\n",
    "    plt.xlabel(df_confusion.columns.name)\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model for prediction\n",
    "\n",
    "Generate predictions for the first 5 images of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "probabilities = model.predict(test_images[0:5])\n",
    "predicted_labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "for i in range(predicted_labels.shape[0]):\n",
    "    print('label:', test_labels[i], 'prediction:', predicted_labels[i])\n",
    "    print(probabilities[i])\n",
    "    plt.imshow(test_images[i].reshape(28,28), cmap='gray')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some mispredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in range(400):\n",
    "    probabilities = model.predict(np.expand_dims(test_images[i], axis=0))\n",
    "    predicted_labels = np.argmax(probabilities, axis=1)[0]\n",
    "    true_label = test_labels[i]\n",
    "    if predicted_labels != true_label:\n",
    "        print('label:', true_label, 'prediction:', predicted_labels)\n",
    "        print(probabilities)\n",
    "        plt.imshow(test_images[i].reshape(28,28), cmap='gray')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note::** There is dataset from Zalando called [fashion-mnist](https://github.com/zalandoresearch/fashion-mnist) that is a drop-in replacement for the original MNIST dataset but more challenging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
