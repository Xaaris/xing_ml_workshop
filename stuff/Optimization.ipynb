{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A framework to design learning algorithms:\n",
    "\n",
    "$ \\DeclareMathOperator*{\\argmin}{arg\\,min} $\n",
    "$ \\underset{\\theta} \\argmin \\frac{1}{T} \\sum_T l(f(x^{(i)};\\theta), y^{(i)}) + \\lambda \\Omega(\\theta)$\n",
    "\n",
    "* $l(f(x^{(i)};\\theta), y^{(i)})$ is a loss function\n",
    "* $\\Omega(\\theta)$ is a regularizer (penalizes certain values of $\\theta$)\n",
    "\n",
    "Learning is cast as an optimization problem. We are searching for the $\\theta$ that minimizes the expression.\n",
    "\n",
    "Remember that $\\theta$ is the variable of this expression, $x^{(i)}$ and $y^{(i)}$ are fix values from the data set.\n",
    "\n",
    "The expression has two parts. The first is the average of a loss function that compares the output of the network with the actual label. The second is a regularizer that penalizes certain values of $\\theta$.\n",
    "\n",
    "The hyper-parameter $\\lambda$ controls the balance between minimizing the average loss and the regularizer function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By casting the learning problem to an optimization problem we can apply an algorithm like Gradient Descent:\n",
    "\n",
    "* Initialize $\\theta$, where $ \\theta = \\{ W^{(1)}, b^{(1)}, ..., W^{(L+1)}, b^{(L+1)} \\}$\n",
    "* For N iterations\n",
    "  * For each training example $(x^{(i)}, y^{(i)})$\n",
    "    * $\\Delta = -\\nabla_\\theta l(f(x^{(i)};\\theta), y^{(i)}) - \\lambda \\nabla_\\theta \\Omega(\\theta)$\n",
    "    * $ \\theta = \\theta + \\alpha \\Delta $\n",
    " \n",
    "N is a hyper-parameter. $\\alpha$ is the step-size or learning rate, this is also a hyper-parameter.\n",
    "\n",
    "Epoch = iteration over **all** training examples\n",
    "\n",
    "To apply this algorithm to neuronal network training we need:\n",
    "\n",
    " * initialization function\n",
    " * the loss function $l(f(x^{(i)};\\theta), y^{(i)})$\n",
    " * a way to compute the loss gradients $ \\nabla_\\theta l(f(x^{(i)};\\theta), y^{(i)}) $\n",
    " * a way to compute the regularizer gradients $ \\nabla_\\theta \\Omega(\\theta) $ \n",
    " \n",
    "There are several variations to this algorithm:\n",
    " * Gradient Descent: Average the gradients of **all** training examples and than do a single update on $\\theta$\n",
    " * Stochastic Gradient Descent: Calculate the gradient for each training example and update $\\theta$ individually\n",
    " * Mini-batch Gradient Descent: Average the gradients of a batch of training examples and update $\\theta$ with this value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function for classification\n",
    "\n",
    "For classification the neuronal network calculates: \n",
    "\n",
    "$ f(x) = [f(x)_{c_1},...,f(x)_{c_n}]$\n",
    "\n",
    "where $c_i$ represents a class and $f(x)_c = p(y=c \\,|\\ x)$. \n",
    "\n",
    "All $f(x)_c$ sum up to 1.\n",
    "\n",
    "We want to maximize the probability of $y^{(i)}$ given $x^{(i)}$\n",
    "\n",
    "To do this we minimize the **negative log-likelihood** or **negative log-probability**:\n",
    "\n",
    "$ l(f(x),y) = - \\sum_c 1_{(y=c)} log(f(x)_c) = -log(f(x)_y)$\n",
    "\n",
    "So the loss for a sample is the negative log of the y-th element of the output vector (this assumes y is 0-based).\n",
    "\n",
    "The log is used to improve numerical stability and math simplicity. Maximizing a value z is the same as maximizing log(z) because it is a monotonical increasing function. We use the negative log because we want to minimize the loss. Minimizing a value -z is the same as maximizing z.\n",
    "\n",
    "This loss function is also known as **cross-entropy**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
