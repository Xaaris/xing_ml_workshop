{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Log loss, cross-entropy loss or negative log probability\n",
    "\n",
    "The cross-entropy loss for a single example:\n",
    "\n",
    "$L = -y \\ log(\\hat{y})$\n",
    "\n",
    "$y$: one-hot encoded true label\n",
    "\n",
    "$\\hat{y}$: predicted probability distribution, usually the output of a softmax layer\n",
    "\n",
    "**Note:** Because $y$ is one-hot encoded the loss only depends on the prediction of the true label. This is a key feature of cross-entropy loss, it rewards/penalizes probabilities of the correct class only. The value is independent of how the remaining probabilities are split between incorrect classes.\n",
    "\n",
    "The loss is 0.0 if the predicted probability is 1.0 and goes to infinity as the probability approaches 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0408219945203\n",
      "4.60517018599\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L(y, y_hat):\n",
    "    '''\n",
    "    Calculate cross entropy loss for a single sample\n",
    "        y : one-hot encoded true label\n",
    "        y_hat : predicted probability distribution\n",
    "    '''\n",
    "    return np.dot(-y, np.log(y_hat))\n",
    "\n",
    "# one-hot encoded label\n",
    "y = np.array([1, 0, 0, 0, 0])\n",
    "\n",
    "# y_hat is a prediction result from a softmax layer\n",
    "\n",
    "y_hat = np.array([0.96, 0.01, 0.01, 0.01, 0.01])\n",
    "print(L(y, y_hat))\n",
    "\n",
    "y_hat = np.array([0.01, 0.2475, 0.2475, 0.2475, 0.2575])\n",
    "print(L(y, y_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The cross-entropy loss for a batch is the average loss of all examples:\n",
    "\n",
    "$ J = -\\frac{1}{N} \\sum_{i=1}^N (y_i \\ log(\\hat{y}_i) )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1273743538747145"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0],    \n",
    "])\n",
    "\n",
    "y_hat = np.array([\n",
    "    [0.1, 0.5, 0.1, 0.1, 0.2],\n",
    "    [0.05, 0.8, 0.05, 0.05, 0.05],\n",
    "    [0.8, 0.05, 0.05, 0.05, 0.05],\n",
    "    [0.9, 0.25, 0.25, 0.25, 0.25],\n",
    "    [0.99, 0.025, 0.025, 0.025, 0.025]\n",
    "])\n",
    "\n",
    "np.mean(-(y * np.log(y_hat)).sum(axis=1))\n",
    "\n",
    "# Note: this implementation is pretty inefficient. First the log of all values is calculated, than the multiplication\n",
    "# with the one-hot encoded y acts like an indicator function that throws away most of the computation. Than all the \n",
    "# sparse row are reduced to a column vector by summing all the 0 values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note about Tensorflow\n",
    "\n",
    "TF has optimized and numerical stable functions that combine softmax/sigmoid with cross-entropy. A TF implementation of cross-entropy would look like this:\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "    \n",
    "This example and more discussions are from [Tensorflow #2462](https://github.com/tensorflow/tensorflow/issues/2462). There is a warning **not** to use this code because it is numerical not stable (!?)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
