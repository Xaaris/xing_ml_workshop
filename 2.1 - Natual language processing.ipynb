{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is the intersection of computer science, linguistics and machine learning that is concerned with the processing of natural language.\n",
    "\n",
    "NLP is all about enabling computers to understand and generate human language. \n",
    "\n",
    "NLP is generally divided into the following fields:\n",
    "\n",
    " * **Speech Recognition** - The translation of spoken language into text\n",
    " * **Natural Language Understanding** - A computers ability to understand text\n",
    " * **Natural Language Generation** - The automatic generation of natural language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Base techniques of text processing\n",
    "\n",
    "### Parsing\n",
    "\n",
    "Parsing refers to the formal analysis of a sentence by a computer into its component parts.\n",
    "\n",
    "The results in a parse tree that shows syntactic relation of the parts to each other.\n",
    "\n",
    "Here is an example of a parse tree for the sentence: \"The thief robbed the appartment\":\n",
    "\n",
    "<img src=\"images/parse-tree.jpg\" height=\"350\" width=\"500\"/> \n",
    "\n",
    "The words are annotated with part of speech tags: noun, verb, determiner etc.\n",
    "\n",
    "Subsequences of words are grouped together: \n",
    "\n",
    " * \"the thief\" is a noun phrase\n",
    " * \"robbed the apartment\" is a verb phrase\n",
    " * the whole structure forms a sentence\n",
    "\n",
    "### Text segmentation\n",
    "\n",
    "Text segmentation is the process of detecting word and sentence boundaries. \n",
    "\n",
    "Detecting word boundaries by splitting at whitespaces is not reliable, take for example the variantes \"ice box\" or \"ice-box\".\n",
    "\n",
    "Detecting sentence boundaries based on certain punctuations (e.g. \".\", \"!\", \"?\" etc.) is likewise not reliable. \n",
    "\n",
    "### Vocabulary\n",
    "\n",
    "The vocabulary is a mapping of words to integer IDs. Normally only word contained in the vocabulary are used for model training or predictions. \n",
    "\n",
    "So called `stop words` are usually removed from the dictionary. Stop words are words that do not contain significant information.\n",
    "\n",
    "Often the vocabulary also maintains a count in how many documents a word occurs. This allows to remove the most frequent and least frequent words. The most frequent words appear in many documents and therefore do not carry much information.\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "Lemmatization is the process of reducing words to their base form. \n",
    "\n",
    "For example \"touch\" is the base form of \"touching\", \"touched\" etc.\n",
    "\n",
    "Lemmatization is useful because we encounter different variations of words that actually have the same base form and meaning.\n",
    "\n",
    "Lemmatization helps to reduce the size of our working vocabulary without losing any information. Typical size reduction are by a factor of 2 or 3.\n",
    "\n",
    "In general reducing the amount of data reduces processing time. For some machine learning problems lemmatization helps to focus the training on a smaller number of classes.\n",
    "\n",
    "Lemmatization is also called stemming.\n",
    "\n",
    "### Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) annotates parts of text with pre-defined categories like:\n",
    "\n",
    " * person\n",
    " * location (city, country etc.)\n",
    " * organization\n",
    " * numeric values (monetary value, percentage, date etc.)\n",
    " \n",
    "NER injects additional semantic into a text dataset.\n",
    "\n",
    "NER also helps to detect words that belong together and fuse them. For example if the consecutive words \"New\" and \"York\" are annotated with CITY they should be fused to \"New York\" and be included as separate word in the vocabulary.\n",
    "\n",
    "### Relationship extraction\n",
    "\n",
    "Relationship Extraction takes the named entities that result from NER and tries to identify the semantic relationships between them. \n",
    "\n",
    "This could mean for example\n",
    "\n",
    " * who is married to whom\n",
    " * a person works for a specific company\n",
    " * a user likes or dislikes a specific product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning tasks for NLP\n",
    "\n",
    "Some examples what kind of NLP problems are solved using machine learning:\n",
    "\n",
    " * Part-of-speech tagging - Assign a POS tag to each token in a document\n",
    " * Named entity extraction - Assign Named Entity tags to each token in the document\n",
    " * Machine translation - Translate a text written in a language to one or more other languages\n",
    " * Document clustering - Group documents by some criteria, e.g. similar content, documents are used together etc.\n",
    " * Document classification - Assign a category to a document\n",
    " * Text summarization - Create a short summary for a text of variable length \n",
    " * Keyword tagging - Assign a set of tags to a document, e.g. to support document organization or indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text representation\n",
    "\n",
    "When we feed text to a machine learning model it must be encoded as a **numeric tensor**.\n",
    "\n",
    "In any case we will have a **vocabulary** that maps words to word IDs. \n",
    "\n",
    "The vocabulary is usually a by-product of preprocessing the document dataset. \n",
    "\n",
    "One interesting aspect of text representation is whether it is capable of capturing **word semantic**.\n",
    "\n",
    "Examples of word semantic:\n",
    "\n",
    " * synonyms: strong, powerful, mighty\n",
    " * hierarchies: company, department, team\n",
    " * word classes like color: blue, red, green\n",
    " * semantic relations like country/language: France/french, Spain/spanish etc.\n",
    " \n",
    "### One-hot encoding\n",
    "\n",
    "Words are encoded as one-hot vectors:\n",
    "\n",
    " * Dimensionality of one-hot vector is same as number of words in the vocabulary\n",
    " * The vector contains only zeros except the index that represents the word is set to 1\n",
    "\n",
    "Documents are the union of the vectors of the words they contain.\n",
    "\n",
    "In general one-hot encoding is a poor representation:\n",
    "\n",
    " * one-hot vectors are sparse, little overlap between different documents\n",
    " * does not capture semantic\n",
    " * the ordering of words is lost\n",
    "\n",
    " \n",
    "### TFIDF\n",
    "\n",
    "[Term frequency–inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (TFIDF), is a score that reflects how important a word is to a document in a collection.\n",
    "\n",
    "The TFIDF value increases with the number of times a word appears in the document.\n",
    "\n",
    "It decreases by the number of documents in the corpus that contain the word\n",
    "\n",
    "TFIDF is based on the following assumptions:\n",
    "\n",
    " * The term appearing often in the document may be more important for identification than the term appearing rarely.\n",
    " * If a term appears in many documents, it will be probably irrelevant.\n",
    "\n",
    "A document is represented by a vector that contains the TFIDF values for all words in the vocabulary.\n",
    "\n",
    "TFIDF is in general a better representation as one-hot encoding but still has the same problems:\n",
    "\n",
    " * vectors are very large and sparse\n",
    " * TFIDF is based on frequency counts and does not capture semantic\n",
    " * word order is lost\n",
    "\n",
    "### Word embeddings\n",
    "\n",
    "Word embeddings represent words as fixed size vectors of 100 to 300 dimensions.\n",
    "\n",
    "Words with similar meaning are mapped to a similar position in the vector space, e.g. they have a smaller distance to each other than words that are less similar.\n",
    "\n",
    "Word embeddings are also able to encode other semantic relationships between words.\n",
    "\n",
    "There are two options to represent document: \n",
    " 1. The word vectors are stored as 2D tensors. The first axis is the word position and the second axis is the embedding dimension.\n",
    " 2. Average the word vectors\n",
    " \n",
    "Both representations usually only work for short documents. \n",
    "\n",
    "### Document embeddings\n",
    "\n",
    "Document embeddings represent whole documents as fixed size vectors of 100 to 300 dimensions.\n",
    "\n",
    "Documents that have similar content are mapped to similar positions in the vector space.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
