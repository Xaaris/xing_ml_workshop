{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model takes a fixes size tensor as input and outputs a tensor:\n",
    " * binary classification: output is a probability, e.g. how likely is the user to read the article\n",
    " * multi-class classification: output is a probability distribution over the class labels, e.g. what category does an article belong to\n",
    " * regression: output is a scalar, e.g. some quality score or audience reach\n",
    "\n",
    "When dealing with natural language, the input x encodes features such as words, part-of-speech tags (noun, verb adjective etc.), entity information (part of a person/organization/country) etc.\n",
    "\n",
    "One-hot encoding\n",
    " * Dimensionality of one-hot vector is same as number of words in the vocabulary\n",
    " * The vector contains only zeros except the index that represents the word is set to 1\n",
    " * The dimensions of the vector are independent of each other\n",
    " \n",
    "Dense vector\n",
    " * Each word is a d-dimensional vector\n",
    " * Similar words will have similar vectors – information is shared between similar words\n",
    " \n",
    "Vector size with one-hot encoding is the size of the vocabulary, that is usually in the range 50k to 200k words. Dense vectors usually have a size of 100 to 300.\n",
    "\n",
    "Using dense representation has a computational advantage. The input data is much smaller, this results in more efficient model training.\n",
    "\n",
    "The main advantage of the dense representations is that is generalizes better, if two words have similar meaning the model is able to capture this semantic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "ML algorithms require the input to be represented as a fixed-length feature vector.\n",
    "\n",
    "When it comes to texts, one of the most common fixed-length features is bag-of-words.\n",
    "\n",
    "One-hot encoding: huge vector, does not capture semantic\n",
    "\n",
    "BOW features have two major weaknesses: \n",
    " * they lose the ordering of the words\n",
    " * they also ignore semantics of the words\n",
    " \n",
    "Paragraph Vector algorithm:\n",
    " * unsupervised\n",
    " * learns from variable-length pieces of texts (sentences, paragraphs, documents etc.)\n",
    " * generates fixed-length representation\n",
    " \n",
    " \n",
    "Text classification and clustering tasks are implemented using logistic regression and k-means.\n",
    "They require fixed length input.\n",
    "Most common fixed-length vector representation for texts is the bag-of-words or bag-of-n-grams.\n",
    "\n",
    "The word order is lost, and thus different sentences can have exactly the same representation, as long as the same words are used.\n",
    "\n",
    "bag-of-n-grams considers the word order in short context but suffers from data sparsity and high dimensionality.\n",
    "\n",
    "Bag-of-words and bag- of-n-grams have very little sense about the semantics of the words or more formally the distances between the words. This means that words “powerful,” “strong” and “Paris” are equally distant despite the fact that semantically, “power- ful” should be closer to “strong” than “Paris.”\n",
    "\n",
    "While paragraph vectors are unique among paragraphs, the word vectors are shared. At prediction time, the paragraph vectors are inferred by fix- ing the word vectors and training the new paragraph vector until convergence.\n",
    "\n",
    "The outcome is that after the model is trained, the word vectors are mapped into a vector space such that semantically similar words have similar vector representa- tions (e.g., “strong” is close to “powerful”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word semantic:\n",
    " * synonyms: strong, powerful, mighty\n",
    " * hierarchies: company, department, team\n",
    " * word classes like color: blue, red, green etc.\n",
    " * semantic relations like country/language: France/french, Spain/spanish etc.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
