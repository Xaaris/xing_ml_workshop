{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Neuronal network uses activation functions for the following reasons:\n",
    "\n",
    " 1. To realize a **non-linear mapping** between the input and output data. Without activation functions a neuronal network would be a linear function and could not learn anything interesting. Think about fitting any dataset with just a straight line.\n",
    " \n",
    " 2. The last activation **transforms the output** so that it is useful for prediction and loss calculation. For example classification problems transform the output to a probability distribution. \n",
    "\n",
    "Overview of popular activation functions:\n",
    "\n",
    "<img src=\"images/activation-functions.png\" height=\"400\" width=\"500\"/>\n",
    "\n",
    "Recall that our MNIST classifier example had the following form:\n",
    "\n",
    "$$\n",
    "z = relu(W_1 \\cdot x + b_1) \\\\\n",
    "y = softmax(W_2 \\cdot z + b_2)\n",
    "$$\n",
    "\n",
    "This model is using the `relu` and `softmax` activation functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit  (ReLU)\n",
    "\n",
    "The Rectified Linear Unit is the most commonly used activation function. \n",
    "\n",
    "$$ f(x)=max(0, x) $$\n",
    "\n",
    "The function returns 0 if it receives any negative input and otherwise just returns the input.\n",
    "\n",
    "ReLU does not require any normalization or exponential computation and is **fast to calculate**, this reduces training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "X = np.linspace(-5, 5, 100)\n",
    "plt.plot(X, ReLU(X),'b')\n",
    "plt.xlabel('X Axis')\n",
    "plt.ylabel('Y Axis')\n",
    "plt.title('ReLU Function')\n",
    "plt.grid()\n",
    "plt.text(3, 0.8, r'$ReLU(x)=max(0.0, x)$', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with ReLU are elements $x_i$ of the input vector $x$ that are always negative. For these elements the ReLU value is always negative. This is called **dying ReLU problem** and should be monitored during training.\n",
    "\n",
    "A solution for that problem are ReLU modifications like `Leaky ReLU` and `Exponential Linear Units (ELUs)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "The [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) squashes real numbers to the interval [0,1].\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "It is usually used as the final activation function for **binary classification problems**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigma(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "X = np.linspace(-5, 5, 100)\n",
    "plt.plot(X, sigma(X),'b')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Sigmoid')\n",
    "plt.grid()\n",
    "plt.text(4, 0.8, r'$\\sigma(x)=\\frac{1}{1+e^{-x}}$', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sigmoid and tanh activation function suffer from a **vanishing gradient problem**. Both end of these curves are almost horizontal. Gradient values in these parts of the curve are very small and gradient based optimization methods do not work very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax activation\n",
    "\n",
    "For a vector $x = [x_0,...,x_n]$ the softmax function calculates:\n",
    "\n",
    "$ softmax(x_i) = \\frac{e^{x_i}}{\\sum_i {e^{x_i}}} $\n",
    "\n",
    "**Note:** In Keras any [activation function](https://keras.io/activations/) can either be used with an Activation layer, or through the activation argument in the layer constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "# Lets assume assume the model has 4 classes\n",
    "\n",
    "# `z` is the output of the layer previous to the Softmax function\n",
    "z = [8.0, 2.0, 9.0, 3.0]\n",
    "\n",
    "probs = softmax(z)\n",
    "print(f'Previous layer output: {z}\\n')\n",
    "\n",
    "print('Model output:')\n",
    "for i in range(probs.shape[0]):\n",
    "    print('P(y=%d|x) = %f' % (i, probs[i]))\n",
    "\n",
    "print('\\nPredicted class:', np.argmax(probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "nb_classes = 4\n",
    "inputs = Input(shape=(nb_classes,), dtype='float32')\n",
    "softmax = Activation('softmax')(inputs)\n",
    "model = Model(inputs=inputs, outputs=softmax)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "# simulate the output of the last layer\n",
    "z = np.array([[8.0, 2.0, 9.0, 3.0]])\n",
    "probs = model.predict(z)\n",
    "\n",
    "for i, prob in enumerate(probs[0]):\n",
    "    print('Probability for label %d: %f' % (i, prob))\n",
    "print('Predicted label:', np.argmax(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only have to calculate the softmax output if you are interested in the probabilities. If you are only interested in the predicted label just determine the index of the largest logit value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
