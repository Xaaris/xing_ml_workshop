{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are currently having:\n",
    "\n",
    " * a **dataset** with examples $x$ and associated target values $y$\n",
    " * a **model** $f(x)$ to that can make prediction given some $x$\n",
    " * a **loss function** L that measure how good model predictions are\n",
    "\n",
    "The next step is to find model **parameters** that result in a low loss over the dataset.\n",
    "\n",
    "Remember the MNIST classification example:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z & = relu(W_1 \\cdot x + b_1) \\\\\n",
    "f(x) & = softmax(W_2 \\cdot z + b_2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this expression, $W_1$, $W_2$, $b_1$ and $b_2$ are the parameters of the model. \n",
    "\n",
    "For convenience we will denote the parameter of a model as $\\theta$. \n",
    "\n",
    "In our example $\\theta = \\{W_1, W_2, b_1, b_2 \\}$.\n",
    "\n",
    "Initially the parameters are filled with random values. \n",
    "\n",
    "Obviously the model will not predict anything useful in this state.\n",
    "\n",
    "**Optimization** refers to the task of minimizing the loss function $L(x, y, \\theta)$ by altering $\\theta$.\n",
    "\n",
    "We'll take advantage of the fact that all model operations are differentiable.\n",
    "\n",
    "We'll compute the **gradient** of the loss function with respect to the model's parameters, this will tell us how to update the parameters to decrease the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def f(x): return x**2 + 3 * x\n",
    "\n",
    "def df_dx(x): return 2 * x + 3\n",
    "\n",
    "x = np.arange(-5, 2, 0.01)\n",
    "plt.xlabel('x')\n",
    "plt.plot(x, f(x), label='f(x)')\n",
    "plt.plot(x, df_dx(x), label='df(x)/dx')\n",
    "plt.axhline(linewidth=1, color='black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a function $f(x)$ has multiple inputs $x = [x_1,...,x_n]$, each input contributes to the value of the function and it does not make sense to have a single derivative. \n",
    "\n",
    "Instead we compute derivatives with respect to one input at a time, these are called **partial derivatives**, but we will not go into more detail at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model optimization\n",
    "\n",
    "Now we can come back to the initial question how to find model parameters $\\theta$ that minimize the loss function $L$ of a model $f(x)$.\n",
    "\n",
    "The loss function takes as input $x$, $y$ and $\\theta$, so we can write it as $L(x, y, \\theta)$.\n",
    "\n",
    "For optimization we must only update the parameters $\\theta$.  $x$ and $y$ are facts that we have observed and that can not be changed.\n",
    "\n",
    "To minimize the loss function we use **gradient descent** with the following update rule:\n",
    "\n",
    "$$ \\theta_{n+1} = \\theta_{n} - \\alpha \\nabla_{\\theta} L(x,y;\\theta_n) $$\n",
    "\n",
    "To train the model we perform the following steps:\n",
    "\n",
    " 1. Initialize $\\theta_0$ with small random values\n",
    " 2. Draw $x$ and $y$ from the dataset\n",
    " 3. Calculate $\\hat y = f(x)$\n",
    " 4. Calculate the gradient of the loss function w.r.t. $\\theta_n$\n",
    " 5. Update all parameters according to the update rule $\\theta_{n+1} = \\theta_{n} - \\alpha \\nabla_\\theta L(x, y;\\theta_{n})$\n",
    " 6. Repeat from step 2 until loss change stays below some threshold\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "The learning rate $\\alpha$ controls how much we are updating the model parameters with respect to the gradient of the loss function.\n",
    "\n",
    "$\\alpha$ is one of the most important hyper-parameters.\n",
    "\n",
    "If $\\alpha$ is set to small gradient descent progress will be slow.\n",
    "\n",
    "If $\\alpha$ is set to large gradient descent may fail to converge.\n",
    "\n",
    "This is a cartoonish depiction of the the effects of different learning rates:\n",
    "\n",
    "<img src=\"images/cartoon_loss.png\" height=\"300\" width=\"400\"/>\n",
    "\n",
    " * With a low learning rate (blue line) the loss will decrease slowly because the parameter updates are very small\n",
    " * High learning rates (green line) will decay the loss faster, but they get stuck at worse values of loss. This is because there is too much \"energy\" in the optimization and the parameters are bouncing around chaotically, unable to settle in a deep spot in the optimization landscape.\n",
    " * With very high learning rate (yellow line) the loss may even grow exponentially\n",
    "\n",
    "One simple solution is to **anneal** the learning rate over time. Starting with a high learning rate helps to speed up the learning progress early on and reducing it over time helps to settle down into deeper but narrower parts of the loss function.\n",
    "\n",
    "Another solution would be to use more sophisticated optimization algorithms that calculate individual learning rates for each model parameter. \n",
    "\n",
    "Examples are:\n",
    "\n",
    " * Adam\n",
    " * RMSProp\n",
    " \n",
    " <img src=\"images/optimizer.gif\" height=\"400\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "\n",
    "You always need to split up your dataset into a **train dataset** and a **test dataset** and keep them separate.\n",
    "\n",
    "The train dataset is used to actually train the model.\n",
    "\n",
    "The test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained.\n",
    "\n",
    "The problem is that during training the model might become to sensitivity to the specific structure of the training data. \n",
    "\n",
    "The model might \"memorize\" the training data but perform poorly on unseen data. This is called **overfitting**.\n",
    "\n",
    "Therefore the model performance is always evaluated on the test dataset that the model has not seen during training.\n",
    "\n",
    "When splitting up the dataset it is important to have an even distribution of target values in both splits.\n",
    "\n",
    "The **split ratio** between the train and test dataset is usually 70/30 to 90/10, depending on the overall datasize.\n",
    "\n",
    "Often a third batch of examples, the **validation set** is used to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting / Underfitting\n",
    "\n",
    "We want a model to **generalize** well, this means to perform well on unseen data.\n",
    "\n",
    "We track training and evaluation accuracy, but evaluation accuracy is the primary measure for model performance.\n",
    "\n",
    "**Overfitting** occurs when a model becomes to sensitive to the specific structure of the training data and does not perform well to unseen data. \n",
    "\n",
    "**Underfitting** occurs when a model cannot adequately capture the underlying structure of the data. Such a model will tend to have poor predictive performance.\n",
    "\n",
    "Here is an example of overfitting / underfitting:\n",
    "\n",
    "<img src=\"images/overfitting.png\" height=\"200\" width=\"300\"/>\n",
    "\n",
    "The example shows a **binary classification problem** that categorizes points as blue or red.\n",
    "\n",
    "The black and green lines are decision boundaries for two different models on the train dataset.\n",
    "\n",
    "The model with the green line has a tendency to overfit. It does a very good job in separating the classes but is too dependent on the train data and is likely to have a higher error rate on unseen data.\n",
    "\n",
    "The model with the the black line has a lower train accuracy but is likely to generalize better.\n",
    "\n",
    "It is a good idea to plot train and test accuracy over time during model training:\n",
    "\n",
    "<img src=\"images/accuracies.jpg\" height=\"200\" width=\"300\"/>\n",
    "\n",
    "The large gap between train accuracy and the blue test accuracy is a clear sign for overfitting.\n",
    "\n",
    "Another sign for overfitting is that the test accuracy gets worse later on.\n",
    "\n",
    "Possible solutions:\n",
    "\n",
    " * reduce the number of parameters in the model\n",
    " * increase the size of the train dataset by collecting more data\n",
    " * increase regularization:\n",
    "     * add dropout layers or increase dropout rate\n",
    "     * stronger L1/L2 weight penalty\n",
    "\n",
    "\n",
    "In general **regularization** is a set to methods that make it harder for the model to learn.\n",
    "\n",
    "The other case is when the test accuracy tracks the train accuracy very well. \n",
    "\n",
    "This is a signal for underfitting.\n",
    "\n",
    "The model capacity is not high enough and learning opportunities are wasted. \n",
    "\n",
    "The solution is to make the model larger: \n",
    "\n",
    " * increasing the number of parameters in existing layers\n",
    " * add more layers\n",
    "\n",
    "In practice you can balance over- and underfitting through experiments. \n",
    "\n",
    "You actually want to see a little bit of overfitting.\n",
    "\n",
    "Stop training early when test accuracy starts to drop and use the model checkpoint with the best performance.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
