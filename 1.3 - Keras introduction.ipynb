{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras](https://keras.io/) is an API that provides high-level building blocks for developing machine learning models.\n",
    "\n",
    "Keras does not implement low level operations like tensor manipulations and differentiation itself but instead delegates them to a backend engine. \n",
    "\n",
    "Several different backend engines can be plugged into Keras:\n",
    "\n",
    " * [TensorFlow (Google)](https://www.tensorflow.org/)\n",
    " * [Theano (MILA lab, Universite of Montreal)](http://deeplearning.net/software/theano/)\n",
    " * [Microsoft Cognitive Toolkit (CNTK)](https://github.com/Microsoft/CNTK)\n",
    "\n",
    "Keras models can be run with any of these backends without having to change the code.\n",
    "\n",
    "Keras is able to run seamlessly on both CPUs and GPUs.\n",
    "\n",
    "A typical deployment stack looks like this: \n",
    " \n",
    "<img src=\"images/keras_stack.png\" height=\"250\" width=\"400\"/> \n",
    "\n",
    " * [CUDA](https://developer.nvidia.com/cuda-toolkit) is a parallel computing API for Nvidia devices\n",
    " * [cuDNN (Deep Neural Network library)](https://developer.nvidia.com/cudnn) is a library that provides primitives for neural networks \n",
    " * [BLAS (Basic Linear Algebra Subprograms)](http://www.netlib.org/blas/) is a library with basic vector and matrix operations\n",
    " * [Eigen](http://eigen.tuxfamily.org/index.php?title=Main_Page) is library for linear algebra\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a Keras model\n",
    "\n",
    "A Keras model contains the following objects:\n",
    "\n",
    " * Layers, which are combined into a model\n",
    " * The input data and labels\n",
    " * The loss function, which defines the feedback signal used for learning\n",
    " * The optimizer, which determines how learning proceeds\n",
    "\n",
    "<img src=\"images/keras_model.png\" height=\"250\" width=\"400\"/> \n",
    "\n",
    "## Layers\n",
    "\n",
    "A layer is a function that takes as input one or more tensors and that outputs one or more tensors.\n",
    "\n",
    "Some layers are stateless, but more frequently layers have a state: the layerâ€™s weights, one or several tensors learned with stochastic gradient descent.\n",
    "\n",
    "Examples of stateless layers:\n",
    " * Dropout: regularization to reducing overfitting in models\n",
    " * Merge layers: concatenate, sum, mean, min, max etc.\n",
    "\n",
    "Examples of stateful layers:\n",
    " * Dense layers\n",
    " * Recurrent layers\n",
    " * Convolution layers\n",
    "\n",
    "Different layers are appropriate for different types of data processing:\n",
    "\n",
    " * Vector data, stored in 2D tensors of shape (batch_size, features), is usually processed by dense layers\n",
    " * Sequence data, stored in 3D tensors of shape (batch_size, timesteps, features), is usually processed by recurrent layers\n",
    " * Image data, stored in 4D tensors of shape (batch_size, height, width, colors), is usually processed by convolution layers\n",
    "\n",
    "You can think of layers as LEGO bricks.\n",
    "\n",
    "Models are built by clipping together compatible layers to form useful data-transformation pipelines.\n",
    "\n",
    "The notion of layer compatibility here refers specifically to the fact that every layer will only accept input tensors of a certain shape and will return output tensors of a certain shape\n",
    "\n",
    "A model is a directed, acyclic graph of layers. \n",
    "\n",
    "The most common instance is a linear stack of layers, mapping a single input to a single output. \n",
    "\n",
    "More complex models will have multiple inputs/outputs or short-cut connections.\n",
    "\n",
    "For each problem class usually exist one or more standard model architectures. \n",
    "\n",
    "It is always a good idea to start with one of this models.\n",
    "\n",
    "In general picking the right model architecture is more an art than a science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers in Keras\n",
    "\n",
    "The following sections demonstrate the function and behavior of some Keras layers. \n",
    "\n",
    "With building a single layer model and running a forward pass (e.g. calling `predict()`) it is possible to introspect the behavior of a layer in isolation. \n",
    "\n",
    "For more information see [Keras layers](https://keras.io/layers/about-keras-layers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense layer\n",
    "\n",
    "A dense layer performs the computation `output = activation(dot(input, W) + b)`.\n",
    "\n",
    "A dense layer takes a tensor of shape (batch_size, input_size) as input and returns a tensor of shape (batch_size, output_size).\n",
    "\n",
    " * `W` is a (input_size, output_size) weight matrix \n",
    " * `b` is a output_size dim. vector\n",
    "\n",
    "Some frequently used [activation functions](https://keras.io/activations) are:\n",
    " * `linear`: identity function, e.g. no activation is applied\n",
    " * `relu`: rectified linear unit\n",
    " * `sigmoid`: Sigmoid function, used in binary classification\n",
    " * `softmax`: softmax function, used in multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (?, 2)\n",
      "Output shape (?, 5)\n",
      "Output: [[ 3.  6.  9. 12. 15.]]\n",
      "Numpy result: [ 3  6  9 12 15]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "W = np.array([\n",
    "    [1,2,3,4,5],\n",
    "    [1,2,3,4,5]])\n",
    "b = np.array([0,0,0,0,0])\n",
    "weights_and_bias = (W, b)\n",
    "\n",
    "inputs = Input(shape=(2,))\n",
    "outputs = Dense(5, activation='linear', weights=weights_and_bias)(inputs) \n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "print('Input shape', model.input.shape)\n",
    "print('Output shape', model.output.shape)\n",
    "\n",
    "x = np.array([1,2])\n",
    "print('Output:', model.predict(np.expand_dims(x, 0)))\n",
    "\n",
    "np_result = np.dot(x, W) + b\n",
    "print('Numpy result:', np_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax activation\n",
    "\n",
    "For a vector $x = [x_0,...,x_n]$ the softmax function calculates:\n",
    "\n",
    "$ softmax(x_i) = \\frac{e^{x_i}}{\\sum_i {e^{x_i}}} $\n",
    "\n",
    "**Note:** In Keras any [activation function](https://keras.io/activations/) can either be used with an Activation layer, or through the activation argument in the layer constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability for label 0: 0.268276\n",
      "Probability for label 1: 0.000665\n",
      "Probability for label 2: 0.729251\n",
      "Probability for label 3: 0.001808\n",
      "Predicted label: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "nb_classes = 4\n",
    "inputs = Input(shape=(nb_classes,), dtype='float32')\n",
    "softmax = Activation('softmax')(inputs)\n",
    "model = Model(inputs=inputs, outputs=softmax)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "# simulate the output of the last layer\n",
    "logits = np.array([[8.0, 2.0, 9.0, 3.0]])\n",
    "probs = model.predict(logits)\n",
    "\n",
    "for i, prob in enumerate(probs[0]):\n",
    "    print('Probability for label %d: %f' % (i, prob))\n",
    "print('Predicted label:', np.argmax(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only have to calculate the softmax output if you are interested in the probabilities. If you are only interested in the predicted label just determine the index of the largest logit value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy loss\n",
    "\n",
    "Input to the cross entropy function **must** be a probability distribution.\n",
    "\n",
    "$ cross\\_entropy(y\\_true, y\\_pred) = -log(y\\_pred_{y\\_true}) $\n",
    "\n",
    "There are multiple implementations of cross entropy:\n",
    " * categorical vs. binary\n",
    " * sparse vs. one-hot encoded\n",
    " \n",
    "In Tensorflow you usually use optimized functions that combine softmax and cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy loss: 4.6051702\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "nb_classes = 5\n",
    "y_true = K.variable(value=np.array([1]), dtype='float32')\n",
    "y_pred = K.variable(value=np.array([[0.01, 0.01, 0.96, 0.01, 0.01]]), dtype='float32')\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "loss = K.eval(loss_fn)\n",
    "print('cross entropy loss:', loss[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
